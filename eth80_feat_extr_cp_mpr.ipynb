{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import functools\n",
    "import itertools\n",
    "import operator\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "\n",
    "from IPython import display\n",
    "with __import__('importnb').Notebook():\n",
    "    # github.com/deathbeds/importnb\n",
    "    import eth80\n",
    "    from eth80 import Eth80Dataset\n",
    "    from feature_extraction import TuckerFeatureExtractor\n",
    "\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import partial_tucker\n",
    "import tensorly.tenalg as ta\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as tnnf\n",
    "from torch.utils.data import TensorDataset\n",
    "from ignite.engine import (\n",
    "    Events, Engine, create_supervised_trainer, create_supervised_evaluator\n",
    ")\n",
    "from ignite.metrics import Accuracy\n",
    "\n",
    "from libcrap import save_json, load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.set_backend(\"pytorch\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth80_dataset = Eth80Dataset(\n",
    "    \"/mnt/hdd_1tb/smiles_backup/Documents/datasets/eth80/eth80-cropped-close128/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPRLowRankCP(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, polynom_order, rank, bias=True):\n",
    "        assert polynom_order >= 1\n",
    "        assert rank >= 1\n",
    "        super().__init__()\n",
    "        self.polynom_order = polynom_order\n",
    "        self.rank = rank\n",
    "        self.bias = bias\n",
    "        self.num_outputs = num_outputs\n",
    "        self.actual_num_inputs = num_inputs + bias\n",
    "        self.factors = nn.ParameterList(\n",
    "            nn.Parameter(\n",
    "                torch.ones(self.num_outputs, self.actual_num_inputs, self.rank),\n",
    "                requires_grad=True\n",
    "            )\n",
    "            for i in range(self.polynom_order)\n",
    "        )\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def _actual_num_features(self):\n",
    "        return self.num_features + bias\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for i in range(len(self.factors)):\n",
    "            self.factors[i].data = torch.randn_like(self.factors[i].data)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        assert len(X.shape) == 2\n",
    "        num_samples = X.shape[0]\n",
    "        if self.bias:\n",
    "            X = torch.cat([X, torch.ones(num_samples, 1)], dim=1)\n",
    "        thingies = [\n",
    "            # n - num of sample, f - num o feature\n",
    "            # r - num of rank one component, o - num of output\n",
    "            torch.einsum(\"nf,ofr->nor\", X, factor)\n",
    "            for factor in self.factors\n",
    "        ]\n",
    "        return functools.reduce(operator.mul, thingies).sum(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_configuration():\n",
    "    return {\n",
    "        \"extracted_features_shape\": (\n",
    "            random.randint(1, eth80.NUM_ANGLES // 2), # angles mode\n",
    "            random.randint(1, 3), # channels mode\n",
    "            random.randint(1, eth80.IMAGE_HEIGHT // 4),\n",
    "            random.randint(1, eth80.IMAGE_WIDTH // 4)\n",
    "        ),\n",
    "        \"learning_rate\": 10**random.randint(-13, -1),\n",
    "        \"regularization_coefficient\": 10**random.randint(-8, 8),\n",
    "        \"polynom_order\": random.randint(2, 3),\n",
    "        \"weights_rank\": random.randint(1, 100)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the best learning rate is around 1e-9, 1e-10. Anything greater than 1e-9 produces NaNs :(. Anything smaller is too slow.\n",
    "\n",
    "Aaaand I don't know the best other hyperparameters because it turns out too few of my experiments were with suitable learning rate.\n",
    "\n",
    "extracted features dimensions [18, 3, 29, 10], lr 1.000000e-09, polynomial order 3, regularization coefficient 1.000000e-03, weights_rank 80 - this gave me the best test accuracy so far."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "configuration = {\n",
    "    'extracted_features_shape': [18, 3, 29, 10],\n",
    "    'regularization_coefficient': 1e-2,\n",
    "    \"learning_rate\": 1e-8,\n",
    "    'polynom_order': 2,\n",
    "    'weights_rank': 80\n",
    "}\n",
    "configuration = {\n",
    "    'extracted_features_shape': [18, 3, 29, 10],\n",
    "    'regularization_coefficient': 1e-8,\n",
    "    \"learning_rate\": 1e-12,\n",
    "    'polynom_order': 4,\n",
    "    'weights_rank': 30\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "configuration = {\n",
    "    'extracted_features_shape': [18, 3, 29, 10],\n",
    "    'regularization_coefficient': 1e-3,\n",
    "    \"learning_rate\": 1e-9,\n",
    "    'polynom_order': 3,\n",
    "    'weights_rank': 80\n",
    "}\n",
    "# test accuracies:\n",
    "torch.tensor([\n",
    "    0.73,\n",
    "    0.8125,\n",
    "    0.6,\n",
    "    1.0,\n",
    "    0.73,\n",
    "    0.6,\n",
    "    0.857,\n",
    "    0.928,\n",
    "    0.75\n",
    "])\n",
    "# mean: 0.7786\n",
    "# std: 0.136"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "configuration = {\n",
    "    'extracted_features_shape': [18, 3, 29, 10],\n",
    "    'regularization_coefficient': 1e-3,\n",
    "    \"learning_rate\": 1e-9,\n",
    "    'polynom_order': 3,\n",
    "    'weights_rank': 160\n",
    "}\n",
    "foo = torch.tensor([\n",
    "    train_test(configuration, num_epochs=501, plot_every_num_epochs=200)\n",
    "    for i in range(25)\n",
    "])\n",
    "print(foo.mean()) #0.8261\n",
    "print(foo.std()) #0.0927"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "configuration = {\n",
    "    'extracted_features_shape': [18, 3, 29, 10],\n",
    "    'regularization_coefficient': 1e-3,\n",
    "    \"learning_rate\": 1e-9,\n",
    "    'polynom_order': 3,\n",
    "    'weights_rank': 500\n",
    "}\n",
    "foo = torch.tensor([\n",
    "    train_test(configuration, num_epochs=501, plot_every_num_epochs=200)\n",
    "    for i in range(10)\n",
    "])\n",
    "print(foo.mean()) # tensor(0.8833)\n",
    "print(foo.std()) #tensor(0.0616)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "configuration = {\n",
    "    'extracted_features_shape': [18, 3, 23, 18],\n",
    "    'regularization_coefficient': 1e-1,\n",
    "    \"learning_rate\": 1e-6,\n",
    "    'polynom_order': 2,\n",
    "    'weights_rank': 10\n",
    "}\n",
    "foo = torch.tensor([\n",
    "    train_test(configuration, num_epochs=1001, plot_every_num_epochs=200)\n",
    "    for i in range(1)\n",
    "])\n",
    "print(foo.mean())\n",
    "print(foo.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memreport():\n",
    "    print(f\"\"\"\n",
    "    {torch.cuda.memory_allocated()/1024/1024/1024} Gb allocated\n",
    "    {torch.cuda.memory_cached()/1024/1024/1024} Gb cached\n",
    "    \"\"\")\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            print(type(obj), obj.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#memreport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_X_y_train_test(\n",
    "    dataset, num_test_objects_per_class, extracted_features_shape\n",
    "):\n",
    "    \"\"\"Performs stratified split of ETH80, does feature extraction via\n",
    "    Tucker decomposition. Returns X_train, y_train, X_test, y_test.\"\"\"\n",
    "    \n",
    "    tensor_train, y_train, tensor_test, y_test = dataset.stratified_split(\n",
    "        num_test_objects_per_class, use_torch=True\n",
    "    )\n",
    "    \n",
    "    extractor = TuckerFeatureExtractor(\n",
    "        tensor_train.shape[1:],\n",
    "        extracted_features_shape\n",
    "    )\n",
    "    core_train = extractor.fit_transform(tensor_train)\n",
    "    core_test = extractor.transform(tensor_test)\n",
    "    \n",
    "    X_train = core_train.reshape(core_train.shape[0], -1)\n",
    "    X_test = core_test.reshape(core_test.shape[0], -1)\n",
    "    assert X_train.shape[1] == X_test.shape[1]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_CP_MPR(\n",
    "    num_extracted_features, polynom_order, weights_rank,\n",
    "    optimizer_creator, learning_rate, regularization_coefficient,\n",
    "    betas=None\n",
    "):\n",
    "    model = MPRLowRankCP(\n",
    "        num_extracted_features, eth80.NUM_CLASSES,\n",
    "        polynom_order=polynom_order,\n",
    "        rank=weights_rank,\n",
    "        bias=True\n",
    "    )\n",
    "    \n",
    "    optimizer_additional_parameters = {}\n",
    "    if betas is not None:\n",
    "        optimizer_additional_parameters[\"betas\"] = betas\n",
    "    \n",
    "    optimizer = optimizer_creator(\n",
    "        model.parameters(), lr=learning_rate,\n",
    "        weight_decay=regularization_coefficient,\n",
    "        **optimizer_additional_parameters\n",
    "    )\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_every_num_epochs(num_epochs):\n",
    "    \"\"\"This must be written after @trainer.on, not before.\"\"\"\n",
    "    def decorate(func):\n",
    "        def decorated(engine, *args, **kwargs):\n",
    "            if engine.state.epoch % num_epochs == 0:\n",
    "                return func(engine, *args, **kwargs)\n",
    "        return functools.update_wrapper(decorated, func)\n",
    "    return decorate\n",
    "\n",
    "def train_and_evaluate(\n",
    "    dataset, extracted_features_shape,\n",
    "    model, optimizer,\n",
    "    num_test_objects_per_class,\n",
    "    eval_every_num_epochs, plot_every_num_epochs,\n",
    "    num_epochs\n",
    "):\n",
    "    X_train, y_train, X_test, y_test = extract_X_y_train_test(\n",
    "        dataset, num_test_objects_per_class, extracted_features_shape\n",
    "    )\n",
    "    \n",
    "    trainer = create_supervised_trainer(\n",
    "        model=model, optimizer=optimizer,\n",
    "        loss_fn=tnnf.cross_entropy\n",
    "    )\n",
    "\n",
    "    evaluations_epochs = []\n",
    "    train_log = {\n",
    "        \"losses\": [],\n",
    "        \"accuracies\": []\n",
    "    }\n",
    "    test_log = {\n",
    "        \"losses\": [],\n",
    "        \"accuracies\": []\n",
    "    }\n",
    "    \n",
    "    def evaluate(X, y, log):\n",
    "        model.train(False)\n",
    "        logits = model(X)\n",
    "        loss = tnnf.cross_entropy(logits, y).item()\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        accuracy = (y == predictions).sum().item() / len(y)\n",
    "        log[\"losses\"].append(loss)\n",
    "        log[\"accuracies\"].append(accuracy)\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    @do_every_num_epochs(eval_every_num_epochs)\n",
    "    def evaluate_on_train_and_test(engine):\n",
    "        evaluate(X_train, y_train, train_log)\n",
    "        evaluate(X_test, y_test, test_log)\n",
    "        assert not isinstance(engine.state.epoch, torch.Tensor)\n",
    "        evaluations_epochs.append(engine.state.epoch)\n",
    "    \n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    @do_every_num_epochs(plot_every_num_epochs)\n",
    "    def update_plot(engine):\n",
    "        display.clear_output(wait=True)\n",
    "        fig, axes = plt.subplots(ncols=2, figsize=(14, 5))\n",
    "        axes = axes.flatten()\n",
    "        axes[0].set_title(\"Loss\")\n",
    "        axes[0].plot(evaluations_epochs, train_log[\"losses\"], label=\"train loss\")\n",
    "        axes[0].plot(evaluations_epochs, test_log[\"losses\"], label=\"test loss\")\n",
    "        axes[0].legend()\n",
    "        axes[1].set_title(f\"Accuracy. Test: {test_log['accuracies'][-1]}\")\n",
    "        axes[1].plot(evaluations_epochs, train_log[\"accuracies\"], label=\"train accuracy\")\n",
    "        axes[1].plot(evaluations_epochs, test_log[\"accuracies\"], label=\"test accuracy\")\n",
    "        axes[1].legend()\n",
    "        plt.show()\n",
    "        \n",
    "    trainer.run([(X_train, y_train)], max_epochs=num_epochs)\n",
    "    return model, evaluations_epochs, train_log, test_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_CP_MPR():\n",
    "    extracted_features_shape = [18, 3, 29, 10]\n",
    "    polynom_order = 3\n",
    "    weights_rank=500\n",
    "    model, optimizer = make_CP_MPR(\n",
    "        functools.reduce(operator.mul, extracted_features_shape), polynom_order,\n",
    "        weights_rank,\n",
    "        torch.optim.SGD, learning_rate=1e-9,\n",
    "        regularization_coefficient=1e-3\n",
    "    )\n",
    "    train_and_evaluate(\n",
    "        eth80_dataset, extracted_features_shape,\n",
    "        model, optimizer,\n",
    "        2,\n",
    "        eval_every_num_epochs=3, plot_every_num_epochs=60,\n",
    "        num_epochs=301\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_CP_MPR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
