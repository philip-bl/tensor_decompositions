{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import functools\n",
    "import itertools\n",
    "import operator\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "\n",
    "from IPython import display\n",
    "with __import__('importnb').Notebook():\n",
    "    # github.com/deathbeds/importnb\n",
    "    import eth80\n",
    "    from feature_extraction import TuckerFeatureExtractor, calc_error\n",
    "\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import partial_tucker\n",
    "import tensorly.tenalg as ta\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as tnnf\n",
    "from torch.utils.data import TensorDataset\n",
    "from ignite.engine import (\n",
    "    Events, Engine, create_supervised_trainer, create_supervised_evaluator\n",
    ")\n",
    "from ignite.metrics import Accuracy\n",
    "\n",
    "from libcrap import save_json, load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.set_backend(\"pytorch\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, y, label_encoder, object_classes = eth80.load_eth80(\n",
    "    \"/mnt/hdd_1tb/smiles_backup/Documents/datasets/eth80/eth80-cropped-close128/\",\n",
    "    use_torch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stratified_split(num_test_per_class):\n",
    "#     obj_indices_sorted_by_class = torch.argsort(y)\n",
    "#     test_objects = set()\n",
    "#     for label in range(eth80.NUM_CLASSES):\n",
    "#         obj_indices_in_class = random.choices(\n",
    "#             range(eth80.NUM_OBJECTS_PER_CLASS), k=num_test_per_class\n",
    "#         )\n",
    "#         new_test_objects = obj_indices_sorted_by_class[[\n",
    "#             label*10 + ind_in_class for ind_in_class in obj_indices_in_class\n",
    "#         ]]\n",
    "#         #import pdb; pdb.set_trace()\n",
    "#         test_objects.update(x.item() for x in new_test_objects)\n",
    "#     train_objects = sorted(frozenset(range(eth80.NUM_OBJECTS)) - test_objects)\n",
    "#     test_objects = sorted(test_objects)\n",
    "#     X_train = dataset[train_objects]\n",
    "#     y_train = y[train_objects]\n",
    "#     X_test = dataset[test_objects]\n",
    "#     y_test = y[test_objects]\n",
    "#     return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPRLowRankCP(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, order, rank, bias=True):\n",
    "        assert order >= 1\n",
    "        assert rank >= 1\n",
    "        super().__init__()\n",
    "        self.order = order\n",
    "        self.rank = rank\n",
    "        self.bias = bias\n",
    "        self.num_outputs = num_outputs\n",
    "        self.actual_num_inputs = num_inputs + bias\n",
    "        self.factors = nn.ParameterList(\n",
    "            nn.Parameter(\n",
    "                torch.ones(self.num_outputs, self.actual_num_inputs, self.rank),\n",
    "                requires_grad=True\n",
    "            )\n",
    "            for i in range(self.order)\n",
    "        )\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def _actual_num_features(self):\n",
    "        return self.num_features + bias\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for i in range(len(self.factors)):\n",
    "            self.factors[i].data = torch.randn_like(self.factors[i].data)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        assert len(X.shape) == 2\n",
    "        num_samples = X.shape[0]\n",
    "        if self.bias:\n",
    "            X = torch.cat([X, torch.ones(num_samples, 1)], dim=1)\n",
    "        thingies = [\n",
    "            # n - num of sample, f - num o feature\n",
    "            # r - num of rank one component, o - num of output\n",
    "            torch.einsum(\"nf,ofr->nor\", X, factor)\n",
    "            for factor in self.factors\n",
    "        ]\n",
    "        return functools.reduce(operator.mul, thingies).sum(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_configuration():\n",
    "    return {\n",
    "        \"extracted_features_shape\": (\n",
    "            random.randint(1, eth80.NUM_ANGLES // 2), # angles mode\n",
    "            random.randint(1, 3), # channels mode\n",
    "            random.randint(1, eth80.IMAGE_HEIGHT // 4),\n",
    "            random.randint(1, eth80.IMAGE_WIDTH // 4)\n",
    "        ),\n",
    "        \"learning_rate\": 10**random.randint(-13, -1),\n",
    "        \"regularization_coefficient\": 10**random.randint(-8, 8),\n",
    "        \"polynom_order\": random.randint(2, 3),\n",
    "        \"weights_rank\": random.randint(1, 100)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the best learning rate is around 1e-9, 1e-10. Anything greater than 1e-9 produces NaNs :(. Anything smaller is too slow.\n",
    "\n",
    "Aaaand I don't know the best other hyperparameters because it turns out too few of my experiments were with suitable learning rate.\n",
    "\n",
    "extracted features dimensions [18, 3, 29, 10], lr 1.000000e-09, polynomial order 3, regularization coefficient 1.000000e-03, weights_rank 80 - this gave me the best test accuracy so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(\n",
    "    configuration, num_epochs, eval_every_num_epochs=10, plot_every_num_epochs=50\n",
    "):\n",
    "    dataset_train, y_train, dataset_test, y_test = eth80.stratified_split_torch(\n",
    "        dataset, y, 2\n",
    "    )\n",
    "    \n",
    "    extractor = TuckerFeatureExtractor(\n",
    "        dataset.shape[1:],\n",
    "        configuration[\"extracted_features_shape\"]\n",
    "    )\n",
    "    core_train = extractor.fit_transform(dataset_train)\n",
    "    core_test = extractor.transform(dataset_test)\n",
    "    \n",
    "    X_train = core_train.reshape(core_train.shape[0], -1)\n",
    "    X_test = core_test.reshape(core_test.shape[0], -1)\n",
    "    assert X_train.shape[1] == X_test.shape[1]\n",
    "    num_extracted_features = X_train.shape[1]\n",
    "    \n",
    "    model = MPRLowRankCP(\n",
    "        num_extracted_features, eth80.NUM_CLASSES,\n",
    "        order=configuration[\"polynom_order\"],\n",
    "        rank=configuration[\"weights_rank\"],\n",
    "        bias=True\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(), lr=configuration[\"learning_rate\"],\n",
    "        weight_decay=configuration[\"regularization_coefficient\"]\n",
    "    )\n",
    "    \n",
    "    trainer = create_supervised_trainer(\n",
    "        model=model, optimizer=optimizer,\n",
    "        loss_fn=tnnf.cross_entropy\n",
    "    )\n",
    "\n",
    "    # update plot and save diagnostic information\n",
    "    train_epochs = []\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_epochs = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def evaluate_on_test(engine):\n",
    "        if engine.state.epoch % eval_every_num_epochs == 0:\n",
    "            model.train(False)\n",
    "            test_logits = model(X_test)\n",
    "            test_loss = tnnf.cross_entropy(test_logits, y_test).item()\n",
    "            test_predictions = test_logits.argmax(dim=1)\n",
    "            test_accuracy = (y_test == test_predictions).sum().item() / len(y_test)\n",
    "            \n",
    "            test_epochs.append(engine.state.epoch)\n",
    "            test_losses.append(test_loss)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def evaluate_on_train(engine):\n",
    "        if engine.state.epoch % eval_every_num_epochs == 0:\n",
    "            model.train(False)\n",
    "            train_logits = model(X_train)\n",
    "            train_loss = tnnf.cross_entropy(train_logits, y_train).item()\n",
    "            train_predictions = train_logits.argmax(dim=1)\n",
    "            train_accuracy = (y_train == train_predictions).sum().item() / len(y_train)\n",
    "            \n",
    "            train_epochs.append(engine.state.epoch)\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def update_plot(engine):\n",
    "        if engine.state.epoch % plot_every_num_epochs == 0:\n",
    "            display.clear_output(wait=True)\n",
    "            fig, axes = plt.subplots(ncols=2, figsize=(14, 5))\n",
    "            axes = axes.flatten()\n",
    "            axes[0].set_title(\"Loss\")\n",
    "            axes[0].plot(train_epochs, train_losses, label=\"train loss\")\n",
    "            axes[0].plot(test_epochs, test_losses, label=\"test loss\")\n",
    "            axes[0].legend()\n",
    "            axes[1].set_title(f\"Accuracy. Test: {test_accuracies[-1]}\")\n",
    "            axes[1].plot(train_epochs, train_accuracies, label=\"train accuracy\")\n",
    "            axes[1].plot(test_epochs, test_accuracies, label=\"test accuracy\")\n",
    "            axes[1].legend()\n",
    "            plt.show()\n",
    "    \n",
    "#     @trainer.on(Events.EPOCH_COMPLETED)\n",
    "#     def free_memory(engine):\n",
    "#         gc.collect()\n",
    "        \n",
    "    trainer.run([(X_train, y_train)], max_epochs=num_epochs)\n",
    "    return test_accuracies[-1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "configuration = {\n",
    "    'extracted_features_shape': [18, 3, 29, 10],\n",
    "    'regularization_coefficient': 1e-2,\n",
    "    \"learning_rate\": 1e-8,\n",
    "    'polynom_order': 2,\n",
    "    'weights_rank': 80\n",
    "}\n",
    "configuration = {\n",
    "    'extracted_features_shape': [18, 3, 29, 10],\n",
    "    'regularization_coefficient': 1e-8,\n",
    "    \"learning_rate\": 1e-12,\n",
    "    'polynom_order': 4,\n",
    "    'weights_rank': 30\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "configuration = {\n",
    "    'extracted_features_shape': [18, 3, 29, 10],\n",
    "    'regularization_coefficient': 1e-3,\n",
    "    \"learning_rate\": 1e-9,\n",
    "    'polynom_order': 3,\n",
    "    'weights_rank': 80\n",
    "}\n",
    "# test accuracies:\n",
    "torch.tensor([\n",
    "    0.73,\n",
    "    0.8125,\n",
    "    0.6,\n",
    "    1.0,\n",
    "    0.73,\n",
    "    0.6,\n",
    "    0.857,\n",
    "    0.928,\n",
    "    0.75\n",
    "])\n",
    "# mean: 0.7786\n",
    "# std: 0.136"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "configuration = {\n",
    "    'extracted_features_shape': [18, 3, 29, 10],\n",
    "    'regularization_coefficient': 1e-3,\n",
    "    \"learning_rate\": 1e-9,\n",
    "    'polynom_order': 3,\n",
    "    'weights_rank': 160\n",
    "}\n",
    "foo = torch.tensor([\n",
    "    train_test(configuration, num_epochs=501, plot_every_num_epochs=200)\n",
    "    for i in range(25)\n",
    "])\n",
    "print(foo.mean()) #0.8261\n",
    "print(foo.std()) #0.0927"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "configuration = {\n",
    "    'extracted_features_shape': [18, 3, 29, 10],\n",
    "    'regularization_coefficient': 1e-3,\n",
    "    \"learning_rate\": 1e-9,\n",
    "    'polynom_order': 3,\n",
    "    'weights_rank': 500\n",
    "}\n",
    "foo = torch.tensor([\n",
    "    train_test(configuration, num_epochs=501, plot_every_num_epochs=200)\n",
    "    for i in range(10)\n",
    "])\n",
    "print(foo.mean()) # tensor(0.8833)\n",
    "print(foo.std()) #tensor(0.0616)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "    'extracted_features_shape': [18, 3, 23, 18],\n",
    "    'regularization_coefficient': 1e-1,\n",
    "    \"learning_rate\": 1e-6,\n",
    "    'polynom_order': 2,\n",
    "    'weights_rank': 10\n",
    "}\n",
    "foo = torch.tensor([\n",
    "    train_test(configuration, num_epochs=1001, plot_every_num_epochs=200)\n",
    "    for i in range(1)\n",
    "])\n",
    "print(foo.mean())\n",
    "print(foo.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memreport():\n",
    "    print(f\"\"\"\n",
    "    {torch.cuda.memory_allocated()/1024/1024/1024} Gb allocated\n",
    "    {torch.cuda.memory_cached()/1024/1024/1024} Gb cached\n",
    "    \"\"\")\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            print(type(obj), obj.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#memreport()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
